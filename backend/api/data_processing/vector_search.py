#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SystÃ¨me de recherche vectorielle pour le chatbot Goo-net Pit
Utilise FAISS pour la recherche sÃ©mantique et AWS Bedrock pour les embeddings
"""

import json
import numpy as np
import faiss
import boto3
from typing import Dict, List, Any, Optional, Tuple
import logging
from pathlib import Path
import pickle
from sentence_transformers import SentenceTransformer
import os

logger = logging.getLogger(__name__)

class GoonetVectorSearch:
    """Moteur de recherche vectorielle pour les donnÃ©es Goo-net Pit"""
    
    def __init__(self, 
                 use_bedrock: bool = True,
                 model_name: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"):
        self.use_bedrock = use_bedrock
        self.model_name = model_name
        self.embedding_dimension = 384  # Dimension pour le modÃ¨le MiniLM
        
        # Initialisation des clients
        if use_bedrock:
            try:
                self.bedrock_client = boto3.client('bedrock-runtime', region_name='us-east-1')
                logger.info("Client AWS Bedrock initialisÃ©")
            except Exception as e:
                logger.warning(f"Impossible d'initialiser Bedrock, utilisation du modÃ¨le local: {e}")
                self.use_bedrock = False
        
        if not self.use_bedrock:
            self.embedding_model = SentenceTransformer(model_name)
            self.embedding_dimension = self.embedding_model.get_sentence_embedding_dimension()
            logger.info(f"ModÃ¨le d'embedding local chargÃ©: {model_name}")
        
        # Index FAISS et mÃ©tadonnÃ©es
        self.index = None
        self.metadata = []
        self.articles = []
        self.garages = []
    
    def get_embedding_bedrock(self, text: str) -> np.ndarray:
        """Obtient un embedding via AWS Bedrock (Titan Embeddings)"""
        try:
            body = json.dumps({
                "inputText": text
            })
            
            response = self.bedrock_client.invoke_model(
                modelId="amazon.titan-embed-text-v1",
                body=body,
                contentType="application/json",
                accept="application/json"
            )
            
            response_body = json.loads(response['body'].read())
            embedding = np.array(response_body['embedding'], dtype=np.float32)
            return embedding
            
        except Exception as e:
            logger.error(f"Erreur lors de l'appel Ã  Bedrock: {e}")
            # Fallback vers le modÃ¨le local
            if hasattr(self, 'embedding_model'):
                return self.embedding_model.encode(text)
            else:
                raise e
    
    def get_embedding_local(self, text: str) -> np.ndarray:
        """Obtient un embedding via le modÃ¨le local"""
        return self.embedding_model.encode(text)
    
    def get_embedding(self, text: str) -> np.ndarray:
        """Interface unifiÃ©e pour obtenir des embeddings"""
        if self.use_bedrock:
            return self.get_embedding_bedrock(text)
        else:
            return self.get_embedding_local(text)
    
    def load_data(self, 
                  articles_file: str = "/workspaces/SmarBot/data/json/diagnostic_articles.json",
                  garages_file: str = "/workspaces/SmarBot/data/json/garages.json"):
        """Charge les donnÃ©es JSON"""
        logger.info("Chargement des donnÃ©es...")
        
        with open(articles_file, 'r', encoding='utf-8') as f:
            self.articles = json.load(f)
        
        with open(garages_file, 'r', encoding='utf-8') as f:
            self.garages = json.load(f)
        
        logger.info(f"DonnÃ©es chargÃ©es: {len(self.articles)} articles, {len(self.garages)} garages")
    
    def create_embeddings(self) -> None:
        """CrÃ©e les embeddings pour tous les articles"""
        logger.info("CrÃ©ation des embeddings...")
        
        embeddings = []
        self.metadata = []
        
        for article in self.articles:
            # Texte pour l'embedding : combinaison optimisÃ©e
            embedding_text = self._create_embedding_text(article)
            
            # GÃ©nÃ©ration de l'embedding
            embedding = self.get_embedding(embedding_text)
            embeddings.append(embedding)
            
            # MÃ©tadonnÃ©es pour la recherche
            metadata = {
                'article_id': article['article_id'],
                'type': 'diagnostic_article',
                'vehicle_manufacturer': article['vehicle_info']['manufacturer'],
                'vehicle_model': article['vehicle_info']['model'],
                'vehicle_year': article['vehicle_info']['year'],
                'obd_codes': [code['code'] for code in article['obd_codes']],
                'symptom': article['symptom'],
                'estimated_price': article['estimated_price'],
                'estimated_duration': article['estimated_duration'],
                'embedding_text': embedding_text
            }
            self.metadata.append(metadata)
        
        # CrÃ©ation de l'index FAISS
        embeddings_array = np.array(embeddings, dtype=np.float32)
        
        # Utilisation de IndexFlatIP pour la similaritÃ© cosinus
        self.index = faiss.IndexFlatIP(self.embedding_dimension)
        
        # Normalisation pour la similaritÃ© cosinus
        faiss.normalize_L2(embeddings_array)
        self.index.add(embeddings_array)
        
        logger.info(f"Index FAISS crÃ©Ã© avec {self.index.ntotal} vecteurs")
    
    def _create_embedding_text(self, article: Dict[str, Any]) -> str:
        """CrÃ©e un texte optimisÃ© pour l'embedding"""
        parts = []
        
        # Informations vÃ©hicule
        if article['vehicle_info']['manufacturer']:
            parts.append(f"ãƒ¡ãƒ¼ã‚«ãƒ¼: {article['vehicle_info']['manufacturer']}")
        if article['vehicle_info']['model']:
            parts.append(f"è»Šç¨®: {article['vehicle_info']['model']}")
        if article['vehicle_info']['year']:
            parts.append(f"å¹´å¼: {article['vehicle_info']['year']}å¹´")
        
        # Codes OBD
        if article['obd_codes']:
            obd_info = []
            for code_info in article['obd_codes']:
                obd_info.append(f"{code_info['code']} {code_info['description']}")
            parts.append(f"æ•…éšœã‚³ãƒ¼ãƒ‰: {', '.join(obd_info)}")
        
        # SymptÃ´me
        if article['symptom']:
            parts.append(f"ç—‡çŠ¶: {article['symptom']}")
        
        # Diagnostic
        if article['diagnosis']:
            parts.append(f"è¨ºæ–­: {article['diagnosis']}")
        
        # Solution
        if article['solution']:
            parts.append(f"å¯¾å‡¦æ³•: {article['solution']}")
        
        # RÃ©sumÃ© comme contexte additionnel
        if article['summary']:
            parts.append(f"è¦ç´„: {article['summary']}")
        
        return " | ".join(parts)
    
    def search(self, 
               query: str, 
               k: int = 5,
               min_similarity: float = 0.3) -> List[Dict[str, Any]]:
        """Recherche sÃ©mantique dans les articles"""
        if self.index is None:
            raise ValueError("Index non initialisÃ©. Appelez create_embeddings() d'abord.")
        
        # GÃ©nÃ©ration de l'embedding de la requÃªte
        query_embedding = self.get_embedding(query)
        query_embedding = query_embedding.reshape(1, -1)
        
        # Normalisation pour la similaritÃ© cosinus
        faiss.normalize_L2(query_embedding)
        
        # Recherche
        similarities, indices = self.index.search(query_embedding, k)
        
        # Formatage des rÃ©sultats
        results = []
        for i, (similarity, idx) in enumerate(zip(similarities[0], indices[0])):
            if similarity >= min_similarity:
                article = self.articles[idx]
                metadata = self.metadata[idx]
                
                result = {
                    'rank': i + 1,
                    'similarity': float(similarity),
                    'article': article,
                    'metadata': metadata,
                    'relevance_explanation': self._explain_relevance(query, article, similarity)
                }
                results.append(result)
        
        return results
    
    def _explain_relevance(self, query: str, article: Dict[str, Any], similarity: float) -> str:
        """Explique pourquoi cet article est pertinent"""
        explanations = []
        
        query_lower = query.lower()
        
        # VÃ©rification du vÃ©hicule
        if article['vehicle_info']['manufacturer'] and \
           article['vehicle_info']['manufacturer'].lower() in query_lower:
            explanations.append(f"ãƒ¡ãƒ¼ã‚«ãƒ¼ä¸€è‡´: {article['vehicle_info']['manufacturer']}")
        
        # VÃ©rification des codes OBD
        for code_info in article['obd_codes']:
            if code_info['code'].lower() in query_lower:
                explanations.append(f"æ•…éšœã‚³ãƒ¼ãƒ‰ä¸€è‡´: {code_info['code']}")
        
        # VÃ©rification des symptÃ´mes
        if article['symptom'] and any(word in query_lower for word in article['symptom'].split()):
            explanations.append(f"ç—‡çŠ¶é–¢é€£: {article['symptom']}")
        
        # Score de similaritÃ©
        if similarity > 0.8:
            relevance = "éå¸¸ã«é–¢é€£æ€§ãŒé«˜ã„"
        elif similarity > 0.6:
            relevance = "é–¢é€£æ€§ãŒé«˜ã„"
        elif similarity > 0.4:
            relevance = "é–¢é€£æ€§ãŒã‚ã‚‹"
        else:
            relevance = "éƒ¨åˆ†çš„ã«é–¢é€£"
        
        explanations.append(f"é¡ä¼¼åº¦: {similarity:.3f} ({relevance})")
        
        return " | ".join(explanations)
    
    def find_nearby_garages(self, 
                           location: str = None,
                           vehicle_manufacturer: str = None,
                           service_type: str = None) -> List[Dict[str, Any]]:
        """ã‚¬ãƒ¬ãƒ¼ã‚¸ã®æ¤œç´¢ (ç°¡æ˜“ç‰ˆ - å®Ÿéš›ã«ã¯ã‚ˆã‚Šé«˜åº¦ãªåœ°ç†çš„æ¤œç´¢ãŒå¿…è¦)"""
        filtered_garages = []
        
        for garage in self.garages:
            match_score = 0
            reasons = []
            
            # Localisation
            if location and location in garage['adresse']:
                match_score += 3
                reasons.append(f"åœ°åŸŸä¸€è‡´: {location}")
            
            # SpÃ©cialitÃ© vÃ©hicule
            if vehicle_manufacturer and vehicle_manufacturer in garage.get('specialites', []):
                match_score += 2
                reasons.append(f"ãƒ¡ãƒ¼ã‚«ãƒ¼å°‚é–€: {vehicle_manufacturer}")
            
            # Type de service
            if service_type and service_type in garage.get('services', []):
                match_score += 1
                reasons.append(f"ã‚µãƒ¼ãƒ“ã‚¹å¯¾å¿œ: {service_type}")
            
            if match_score > 0 or not any([location, vehicle_manufacturer, service_type]):
                garage_result = garage.copy()
                garage_result['match_score'] = match_score
                garage_result['match_reasons'] = reasons
                filtered_garages.append(garage_result)
        
        # Tri par score de correspondance
        filtered_garages.sort(key=lambda x: x['match_score'], reverse=True)
        
        return filtered_garages[:5]  # Top 5
    
    def save_index(self, index_dir: str = "/workspaces/SmarBot/data/faiss_index"):
        """Sauvegarde l'index FAISS et les mÃ©tadonnÃ©es"""
        Path(index_dir).mkdir(parents=True, exist_ok=True)
        
        if self.index is not None:
            # Sauvegarde de l'index FAISS
            index_file = Path(index_dir) / "articles.index"
            faiss.write_index(self.index, str(index_file))
            
            # Sauvegarde des mÃ©tadonnÃ©es
            metadata_file = Path(index_dir) / "metadata.pkl"
            with open(metadata_file, 'wb') as f:
                pickle.dump(self.metadata, f)
            
            logger.info(f"Index sauvegardÃ© dans {index_dir}")
    
    def load_index(self, index_dir: str = "/workspaces/SmarBot/data/faiss_index"):
        """Charge l'index FAISS et les mÃ©tadonnÃ©es"""
        index_file = Path(index_dir) / "articles.index"
        metadata_file = Path(index_dir) / "metadata.pkl"
        
        if index_file.exists() and metadata_file.exists():
            self.index = faiss.read_index(str(index_file))
            
            with open(metadata_file, 'rb') as f:
                self.metadata = pickle.load(f)
            
            logger.info(f"Index chargÃ© depuis {index_dir}")
            return True
        return False

if __name__ == '__main__':
    # Test du systÃ¨me de recherche
    search_engine = GoonetVectorSearch(use_bedrock=False)
    
    # Chargement des donnÃ©es
    search_engine.load_data()
    
    # CrÃ©ation des embeddings
    search_engine.create_embeddings()
    
    # Sauvegarde
    search_engine.save_index()
    
    # Test de recherche
    test_queries = [
        "ãƒ›ãƒ³ãƒ€ã®N-BOXã§ãƒãƒ³ãƒ‰ãƒ«ãŒé‡ã„",
        "ãƒˆãƒ¨ã‚¿ ãƒ—ãƒªã‚¦ã‚¹ ã‚¨ãƒ³ã‚¸ãƒ³è­¦å‘Šç¯",
        "U3003 ãƒãƒƒãƒ†ãƒªãƒ¼ ç•°å¸¸",
        "ã‚¨ã‚¢ã‚³ãƒ³ãŒåŠ¹ã‹ãªã„ ä¿®ç†"
    ]
    
    print("\nğŸ” Tests de recherche:")
    for query in test_queries:
        print(f"\nğŸ“ RequÃªte: {query}")
        results = search_engine.search(query, k=3)
        
        for result in results:
            print(f"  ğŸ“„ [{result['rank']}] Article {result['article']['article_id']}")
            print(f"      ğŸ¯ SimilaritÃ©: {result['similarity']:.3f}")
            print(f"      ğŸš— VÃ©hicule: {result['article']['vehicle_info']['manufacturer']} {result['article']['vehicle_info']['model']} ({result['article']['vehicle_info']['year']})")
            print(f"      ğŸš¨ Codes OBD: {[c['code'] for c in result['article']['obd_codes']]}")
            print(f"      ğŸ’¡ Pertinence: {result['relevance_explanation']}")
    
    print("\nâœ… Tests de recherche vectorielle terminÃ©s!")
